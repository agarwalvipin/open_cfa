## üöÄ Module 2: Evaluating Regression Model Fit and Interpreting Model Results üìä

### üéØ Introduction

Welcome back, **Quant Warrior**\! ü•ã In Module 1, we built the engine (the Multiple Regression Model). Now, in Module 2, we need to pop the hood and see if that engine actually runs.

It's not enough to just run a regression; you need to know if it's **reliable**, if you've included too much "junk" data, or if you can actually use it to predict the future. Think of this module as your **Quality Control** phase. We will move beyond simple $R^2$ and learn how to penalize models for being too complex, how to test groups of variables simultaneously, and how to forecast with precision. Let's dive in\! üïµÔ∏è‚Äç‚ôÇÔ∏è

-----

### <span style="color: #1565C0;">Part 1: Goodness of Fit - The Battle of the R-Squares ü•ä</span>

In Simple Linear Regression (Level 1), we loved **Coefficient of Determination (\\(R^2\\))**. It told us the % of variation explained by the model. But in **Multiple Regression**, \\(R^2\\) has a fatal flaw: it's a "people pleaser." 

  * **The Problem:** If you add *any* variable to a model‚Äîeven a useless one like "Phase of the Moon"‚Äî\\(R^2\\) will either stay the same or go up. It never goes down. 
  * **The Consequence:** This leads to **Overfitting**. You might have a high \\(R^2\\), but the model is just memorizing noise, not learning patterns. 

#### <span style="color: #6A1B9A;">1.1 Adjusted \\(R^2\\) (\\(\bar{R}^2\\))</span>

Enter the **Adjusted \\(R^2\\)**. This is the strict teacher who penalizes you for adding unnecessary variables. It adjusts for the degrees of freedom. 

<div style="background-color: #F5F5F5; padding: 10px; border-radius: 5px; margin: 10px 0;">

\\[\\bar{R}^2 = 1 - \\left[ \\left( \\frac{n - 1}{n - k - 1} \\right) (1 - R^2) \\right]\\]

Where:

  * \\(n\\) = number of observations
  * \\(k\\) = number of independent variables

</div>

<div style="background-color: #E3F2FD; border-left: 5px solid #1976D2; padding: 12px; margin: 15px 0;">
<div style="color: #000000; font-weight: 500;">

üí° **CFA Exam Tip ‚úçÔ∏è:** When do you gain from adding a variable?
The Adjusted \\(R^2\\) will **increase** only if the new variable's **t-statistic is greater than 1.0** (absolute value). If \\(|t| < 1.0\\), Adjusted \\(R^2\\) decreases. 

</div>
</div>

#### <span style="color: #6A1B9A;">1.2 Visualizing the Trade-off</span>

This chart illustrates how regular \\(R^2\\) keeps climbing, while Adjusted \\(R^2\\) eventually drops if we add "junk" variables.

<div style="text-align: center;">

<pre data-lang="vega-lite">
{
"$schema": "https://vega.github.io/schema/vega-lite/v5.json",
"title": "R-Squared vs. Adjusted R-Squared",
"width": "container",
"height": 400,
"data": {
"values": [
{"Variables": 1, "Value": 0.50, "Metric": "R-Squared"},
{"Variables": 1, "Value": 0.49, "Metric": "Adj R-Squared"},
{"Variables": 2, "Value": 0.60, "Metric": "R-Squared"},
{"Variables": 2, "Value": 0.58, "Metric": "Adj R-Squared"},
{"Variables": 3, "Value": 0.65, "Metric": "R-Squared"},
{"Variables": 3, "Value": 0.62, "Metric": "Adj R-Squared"},
{"Variables": 4, "Value": 0.66, "Metric": "R-Squared"},
{"Variables": 4, "Value": 0.60, "Metric": "Adj R-Squared"},
{"Variables": 5, "Value": 0.665, "Metric": "R-Squared"},
{"Variables": 5, "Value": 0.58, "Metric": "Adj R-Squared"}
]
},
"mark": {"type": "line", "point": true, "tooltip": true},
"encoding": {
"x": {"field": "Variables", "type": "ordinal", "title": "Number of Independent Variables"},
"y": {"field": "Value", "type": "quantitative", "title": "Goodness of Fit", "scale": {"domain": [0.4, 0.7]}},
"color": {"field": "Metric", "type": "nominal"}
}
}
</pre>

</div>

-----

### <span style="color: #1565C0;">Part 2: Model Selection (AIC & BIC) üèÜ</span>

When comparing multiple models (e.g., Model A has 3 variables, Model B has 5), we use Information Criteria. These metrics balance **Fit** (SSE) vs. **Complexity** (number of variables).

**The Golden Rule:** For both AIC and BIC, **LOWER IS BETTER**. 

#### <span style="color: #6A1B9A;">2.1 The Contenders</span>

1.  **AIC (Akaike‚Äôs Information Criterion):**
      * Use when the goal is **Prediction**. 
2.  **BIC (Schwarz‚Äôs Bayesian Information Criterion):**
      * Use when the goal is **Goodness of Fit** (finding the "true" model).
      * BIC imposes a **heavier penalty** for adding variables than AIC. It prefers simpler (parsimonious) models. 

<div style="background-color: #F3E5F5; border-left: 5px solid #7B1FA2; padding: 12px; margin: 15px 0;">
<div style="color: #000000; font-weight: 500;">

**üí° MNEMONIC: "AIC for Action, BIC for Best"**

  * **A**IC = **A**ction (Predicting/Forecasting)
  * **B**IC = **B**est Fit (Parsimonious/Conservative)

</div>
</div>

-----

### <span style="color: #1565C0;">Part 3: Testing Joint Hypotheses (The F-Test) üß™</span>

Sometimes you want to test if a **group** of variables matters.

  * *Example:* Does the "Fama-French" set of variables (Size + Value) actually add value to a model that already has Market Return? 

To do this, we compare two models using an **F-Test**:

1.  **Unrestricted Model:** The Big Model (contains all variables). 
2.  **Restricted Model:** The Small Model (we force the coefficients of the variables we are testing to be zero‚Äîeffectively removing them). 

#### <span style="color: #6A1B9A;">3.1 The Nested F-Statistic Formula</span>

We check if the Sum of Squared Errors (SSE) increases significantly when we remove the variables.

<div style="background-color: #F5F5F5; padding: 10px; border-radius: 5px; margin: 10px 0;">

\\[F = \\frac{(SSE_{restricted} - SSE_{unrestricted}) / q}{SSE_{unrestricted} / (n - k - 1)}\\]

Where:

  * \\(q\\) = number of restrictions (variables removed)
  * \\(n - k - 1\\) = degrees of freedom of the Unrestricted model

</div>

If the **F-Stat > Critical Value**, the omitted variables **WERE** important. We reject the null (that they are zero). 

#### <span style="color: #6A1B9A;">3.2 Decision Flowchart</span>

```mermaid
graph TD
    A["Start: Compare Big vs. Small Model"] --> B["Calculate F-Statistic<br/>((SSE_res - SSE_un) / q) / MSE_un"]
    B --> C{"Is F-Calc > F-Critical?"}
    C -- "Yes" --> D["Reject Null<br/>üö´ Coefficients are NOT zero"]
    D --> E["Conclusion:<br/>The variables add value.<br/>Keep the Big Model. ‚úÖ"]
    C -- "No" --> F["Fail to Reject Null<br/>ü§∑ Coefficients are zero"]
    F --> G["Conclusion:<br/>The variables are useless.<br/>Use the Restricted Model. üìâ"]
```

-----

### <span style="color: #1565C0;">Part 4: Forecasting with Multiple Regression üîÆ</span>

The ultimate goal is often prediction. The formula looks just like the regression equation, but we plug in our **forecasted** independent variables (\\(X\\)s).

#### <span style="color: #6A1B9A;">4.1 The Formula</span>

<div style="background-color: #F5F5F5; padding: 10px; border-radius: 5px; margin: 10px 0;">

\\[\\hat{Y} = \\hat{b}_0 + \\hat{b}_1 X_1 + \\hat{b}_2 X_2 + \\dots + \\hat{b}_k X_k\\]

</div>

**Crucial Note:** You must use the **intercept** (\\(\hat{b}_0\\)) and **all** coefficients in the calculation, even if their individual t-statistics were low, because the model was estimated using them all together. 

#### <span style="color: #6A1B9A;">4.2 Uncertainty in Forecasting</span>

When we predict \\(\hat{Y}\\), we are dealing with two sources of uncertainty (error):

1.  **Model Error:** The regression model isn't perfect (the residuals).
2.  **Sampling Error:** We estimated the coefficients (\\(b\\)'s) using a sample, not the population. 

Because we have to account for *both* errors, the **Prediction Interval** (for a single new observation) is always **WIDER** than the Confidence Interval (for the mean). 

-----

### <span style="color: #1565C0;">Part 5: Quick Exam-Day Pointers üéØ</span>

<div style="background-color: #FFF9E6; border-left: 5px solid #F57C00; padding: 15px; margin: 20px 0;">

### üéØ Quick Exam-Day Pointers

<div style="color: #000000; font-weight: 500;">

  * **Adj \\(R^2\\) vs \\(R^2\\):** Standard \\(R^2\\) never drops. Adj \\(R^2\\) drops if you add a "weak" variable (t-stat < 1).
  * **Model Selection:** If asked to pick the "best" model for *forecasting*, pick the lowest **AIC**. If asked for the "correct" model (goodness of fit), pick the lowest **BIC**.
  * **Interpretation:** Slope coefficients in multiple regression are "partial" slopes. They measure change in Y holding **all other X's constant**.
  * **F-Test Logic:** If the SSE of the Restricted Model is much *higher* than the Unrestricted Model, it means removing those variables caused a huge error. Therefore, those variables were important (Reject Null).
  * **Forecasting:** Don't forget the intercept\! And remember, prediction intervals are wide because they account for the error term + coefficient uncertainty.

</div>
</div>