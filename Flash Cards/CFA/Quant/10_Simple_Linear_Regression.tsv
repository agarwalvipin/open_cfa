Front	Back	Tags
What is the general form of a simple linear regression model?	\[Y_i = b_0 + b_1 X_i + \epsilon_i\]<br>Where:<br><b>Y<sub>i</sub></b> is the dependent variable.<br><b>X<sub>i</sub></b> is the independent variable.<br><b>b<sub>0</sub></b> is the intercept coefficient.<br><b>b<sub>1</sub></b> is the slope coefficient.<br><b>ε<sub>i</sub></b> is the error term.	Quant_Methods::Module_10::Formula::Definition
How are the coefficients in a simple linear regression model estimated?	They are estimated using the method of <b>ordinary least squares (OLS)</b>, which minimizes the sum of the squared residuals (the differences between the observed and predicted values of the dependent variable).	Quant_Methods::Module_10::Concept
How is the slope coefficient (b₁) interpreted in a simple linear regression?	The slope coefficient (b₁) represents the estimated change in the dependent variable (Y) for a one-unit change in the independent variable (X).	Quant_Methods::Module_10::Concept
How is the intercept coefficient (b₀) interpreted in a simple linear regression?	The intercept coefficient (b₀) is the estimated value of the dependent variable (Y) when the independent variable (X) is zero.	Quant_Methods::Module_10::Concept
What are the four key assumptions of the simple linear regression model?	1. <b>Linearity:</b> The relationship between X and Y is linear.<br>2. <b>Homoskedasticity:</b> The variance of the residuals is constant for all observations.<br>3. <b>Independence:</b> The residuals are uncorrelated across observations.<br>4. <b>Normality:</b> The residuals are normally distributed.	Quant_Methods::Module_10::Concept
What is homoskedasticity, and what is its opposite?	<b>Homoskedasticity</b> means that the variance of the residuals is constant across all observations.<br><b>Heteroskedasticity</b> is the violation of this assumption, meaning the variance of the residuals is not constant. This can often be detected by a funnel shape in a residual plot.	Quant_Methods::Module_10::Definition
What is the Coefficient of Determination (R²)?	R² measures the proportion of the total variation in the dependent variable that is explained by the independent variable. It ranges from 0 to 1, with higher values indicating a better fit of the model.<br><br>\[R^2 = \frac{\text{Sum of Squares Regression (SSR)}}{\text{Sum of Squares Total (SST)}}\]	Quant_Methods::Module_10::Formula::Definition
What is the relationship between the coefficient of determination (R²) and correlation (r) in a simple linear regression?	In a simple linear regression, the coefficient of determination is equal to the square of the correlation coefficient between the dependent and independent variables.<br><br>\[R^2 = r^2\]	Quant_Methods::Module_10::Concept
What is the Standard Error of the Estimate (SEE)?	The SEE is the standard deviation of the regression residuals. It is an absolute measure of the goodness of fit of the model, indicating the typical distance between the observed values and the values predicted by the regression line.<br><br>\[s_e = \sqrt{\text{Mean Square Error (MSE)}}\]	Quant_Methods::Module_10::Formula::Definition
What is Analysis of Variance (ANOVA) in the context of regression?	ANOVA is a statistical procedure that decomposes the total variation in the dependent variable into its explained and unexplained components.<br><br>\[\text{Sum of Squares Total (SST)} = \text{Sum of Squares Regression (SSR)} + \text{Sum of Squares Error (SSE)}\]	Quant_Methods::Module_10::Concept
What does the F-statistic in a regression test?	The F-statistic tests the overall significance of the regression model. In a simple linear regression, it tests the null hypothesis that the slope coefficient is equal to zero (H₀: b₁ = 0). A significant F-statistic indicates that the independent variable is a significant predictor of the dependent variable.	Quant_Methods::Module_10::Concept
What is a prediction interval?	A prediction interval is a range of values that is likely to contain the actual value of a single new observation of the dependent variable, given a specific value of the independent variable. It accounts for both the uncertainty in the model's parameters and the random error term.	Quant_Methods::Module_10::Definition
What is the log-lin functional form in regression?	A log-lin model uses the natural logarithm of the dependent variable and the linear value of the independent variable. <br><br>\[\ln(Y_i) = b_0 + b_1 X_i + \epsilon_i\]<br>In this model, the slope coefficient b₁ represents the relative (percentage) change in Y for an absolute change in X.	Quant_Methods::Module_10::Formula
What is the lin-log functional form in regression?	A lin-log model uses the linear value of the dependent variable and the natural logarithm of the independent variable.<br><br>\[Y_i = b_0 + b_1 \ln(X_i) + \epsilon_i\]<br>In this model, the slope coefficient b₁ represents the absolute change in Y for a relative (percentage) change in X.	Quant_Methods::Module_10::Formula
What is the log-log functional form in regression?	A log-log model uses the natural logarithm of both the dependent and independent variables.<br><br>\[\ln(Y_i) = b_0 + b_1 \ln(X_i) + \epsilon_i\]<br>In this model, the slope coefficient b₁ represents the elasticity, which is the percentage change in Y for a percentage change in X.	Quant_Methods::Module_10::Formula